###Spark电商用户行为分析
####模块一：用户访问session分析
> 用户访问session介绍:
  用户在电商网站上,通常会有很多的点击行为,通常都是进入首页;然后可能点击首页上的一些商品;点击首页上的一些品类;也可能随时在搜索框里面搜索关键词;还可能将一些商品加入购物车;对购物车中的多个商品下订单;最后对订单中的多个商品进行支付.
  用户的每一次操作,其实可以理解为一个action,比如点击,搜索,下单,支付.
  用户session,指的就是:从用户第一次进入首页,session就开始了,然后在一定时间范围内,直到最后操作完(可能做了几十次、甚至上百次操作).离开网站,关闭浏览器,或者长时间没有做操作,那么session就结束了.
  以上用户在网站内的访问过程,就称之为一次session.**简单理解,session就是某一天某一个时间段内,某个用户对网站从打开/进入,到做了大量操作,到最后关闭浏览器的过程,就叫做session.**    
  session实际上就是一个电商网站中最基本的数据和大数据.那么大数据,面向C端,也就是customer,消费者,用户端的,分析,基本是最基本的就是面向用户访问行为/用户访问session.

> 在实际企业项目中的使用架构:
  1.J2EE的平台,通过这个J2EE平台可以让使用者,提交各种各样的分析任务,其中就包括一个模块就是用户访问session分析模块;可以指定各种各样的筛选条件,比如年龄范围、职业、城市等等.
  2.J2EE平台接收到了执行统计分析任务的请求之后,会调用底层的封装了spark-submit的shell脚本(Runtime、Process),shell脚本进而提交我们编写的Spark作业.
  3.Spark作业获取使用者指定的筛选参数,然后运行复杂的作业逻辑,进行该模块的统计和分析.
  4.Spark作业统计和分析的结果,会写入MySQL中指定的表.
  5.最后,J2EE平台使用者可以通过前端页面,以表格、图表的形式展示和查看MySQL中存储的该统计分析任务的结果数据.
  
模块的目标：对用户访问session进行分析
1、可以根据使用者指定的某些条件，筛选出指定的一些用户（有特定年龄、职业、城市）；
2、对这些用户在指定日期范围内发起的session，进行聚合统计，比如，统计出访问时长在0~3s的session占总session数量的比例；
3、按时间比例，比如一天有24个小时，其中12:00~13:00的session数量占当天总session数量的50%，当天总session数量是10000个，那么当天总共要抽取1000个session，ok，12:00~13:00的用户，就得抽取1000*50%=500。而且这500个需要随机抽取。
4、获取点击量、下单量和支付量都排名10的商品种类
5、获取top10的商品种类的点击数量排名前10的session
6、开发完毕了以上功能之后，需要进行大量、复杂、高端、全套的性能调优
7、十亿级数据量的troubleshooting（故障解决）的经验总结
8、数据倾斜的完美解决方案
9、使用mock（模拟）的数据，对模块进行调试、运行和演示效果

#####数据表
表名：user_visit_action（Hive表）
```
date：日期，代表这个用户点击行为是在哪一天发生的
user_id：代表这个点击行为是哪一个用户执行的
session_id ：唯一标识了某个用户的一个访问session
page_id ：点击了某些商品/品类，也可能是搜索了某个关键词，然后进入了某个页面，页面的id
action_time ：这个点击行为发生的时间点
search_keyword ：如果用户执行的是一个搜索行为，比如说在网站/app中，搜索了某个关键词，然后会跳转到商品列表页面；搜索的关键词
click_category_id ：可能是在网站首页，点击了某个品类（美食、电子设备、电脑）
click_product_id ：可能是在网站首页，或者是在商品列表页，点击了某个商品（比如呷哺呷哺火锅XX路店3人套餐、iphone 6s）
order_category_ids ：代表了可能将某些商品加入了购物车，然后一次性对购物车中的商品下了一个订单，这就代表了某次下单的行为中，有哪些商品品类，可能有6个商品，但是就对应了2个品类，比如有3根火腿肠（食品品类），3个电池（日用品品类）
order_product_ids ：某次下单，具体对哪些商品下的订单
pay_category_ids ：代表的是，对某个订单，或者某几个订单，进行了一次支付的行为，对应了哪些品类
pay_product_ids：代表的，支付行为下，对应的哪些具体的商品
```
**user_visit_action表，其实就是放，比如说网站，或者是app，每天的点击流的数据。可以理解为，用户对网站/app每点击一下，就会代表在这个表里面的一条数据。**

表名：user_info（Hive表）
```
user_id：其实就是每一个用户的唯一标识，通常是自增长的Long类型，BigInt类型
username：是每个用户的登录名
name：每个用户自己的昵称、或者是真实姓名
age：用户的年龄
professional：用户的职业
city：用户所在的城市
```

表名：task（MySQL表）
```
task_id：表的主键
task_name：任务名称
create_time：创建时间
start_time：开始运行的时间
finish_time：结束运行的时间
task_type：任务类型，就是说，在一套大数据平台中，肯定会有各种不同类型的统计分析任务，比如说用户访问session分析任务，页面单跳转化率统计任务；所以这个字段就标识了每个任务的类型
task_status：任务状态，任务对应的就是一次Spark作业的运行，这里就标识了，Spark作业是新建，还没运行，还是正在运行，还是已经运行完毕
task_param：最最重要，用来使用JSON的格式，来封装用户提交的任务对应的特殊的筛选参数
```
> task表，其实是用来保存平台的使用者，通过J2EE系统，提交的基于特定筛选参数的分析任务，的信息，就会通过J2EE系统保存到task表中来。之所以使用MySQL表，是因为J2EE系统是要实现快速的实时插入和查询的。

> 1、按条件筛选session
  2、统计出符合条件的session中，访问时长在1s~3s、4s~6s、7s~9s、10s~30s、30s~60s、1m~3m、3m~10m、10m~30m、30m以上各个范围内的session占比；访问步长在1~3、4~6、7~9、10~30、30~60、60以上各个范围内的session占比
  3、在符合条件的session中，按照时间比例随机抽取1000个session
  4、在符合条件的session中，获取点击、下单和支付数量排名前10的品类
  5、对于排名前10的品类，分别获取其点击次数排名前10的session
    
  1、按条件筛选session
  
  搜索过某些关键词的用户、访问时间在某个时间段内的用户、年龄在某个范围内的用户、职业在某个范围内的用户、所在某个城市的用户，发起的session。找到对应的这些用户的session，也就是我们所说的第一步，按条件筛选session。
  
  这个功能，就最大的作用就是灵活。也就是说，可以让使用者，对感兴趣的和关系的用户群体，进行后续各种复杂业务逻辑的统计和分析，那么拿到的结果数据，就是只是针对特殊用户群体的分析结果；而不是对所有用户进行分析的泛泛的分析结果。比如说，现在某个企业高层，就是想看到用户群体中，28~35岁的，老师职业的群体，对应的一些统计和分析的结果数据，从而辅助高管进行公司战略上的决策制定。
  
  2、统计出符合条件的session中，访问时长在1s~3s、4s~6s、7s~9s、10s~30s、30s~60s、1m~3m、3m~10m、10m~30m、30m以上各个范围内的session占比；访问步长在1~3、4~6、7~9、10~30、30~60、60以上各个范围内的session占比
  
  session访问时长，也就是说一个session对应的开始的action，到结束的action，之间的时间范围；还有，就是访问步长，指的是，一个session执行期间内，依次点击过多少个页面，比如说，一次session，维持了1分钟，那么访问时长就是1m，然后在这1分钟内，点击了10个页面，那么session的访问步长，就是10.
  
  比如说，符合第一步筛选出来的session的数量大概是有1000万个。那么里面，我们要计算出，访问时长在1s~3s内的session的数量，并除以符合条件的总session数量（比如1000万），比如是100万/1000万，那么1s~3s内的session占比就是10%。依次类推，这里说的统计，就是这个意思。
  
  这个功能的作用，其实就是，可以让人从全局的角度看到，符合某些条件的用户群体，使用我们的产品的一些习惯。比如大多数人，到底是会在产品中停留多长时间，大多数人，会在一次使用产品的过程中，访问多少个页面。那么对于使用者来说，有一个全局和清晰的认识。
  
  3、在符合条件的session中，按照时间比例随机抽取1000个session
  
  随机抽取本身是很简单的，但是按照时间比例，就很复杂了。比如说，这一天总共有1000万的session。那么我现在总共要从这1000万session中，随机抽取出来1000个session。但是这个随机不是那么简单的。需要做到如下几点要求：首先，如果这一天的12:00~13:00的session数量是100万，那么这个小时的session占比就是1/10，那么这个小时中的100万的session，我们就要抽取1/10 * 1000 = 100个。然后再从这个小时的100万session中，随机抽取出100个session。以此类推，其他小时的抽取也是这样做。
    
  之所以要做到按时间比例随机采用抽取，就是要做到，观察样本的公平性。
  
  4、在符合条件的session中，获取点击、下单和支付数量排名前10的品类
  
  什么意思呢，对于这些session，每个session可能都会对一些品类的商品进行点击、下单和支付等等行为。那么现在就需要获取这些session点击、下单和支付数量排名前10的最热门的品类。也就是说，要计算出所有这些session对各个品类的点击、下单和支付的次数，然后按照这三个属性进行排序，获取前10个品类。
  
  这个功能，很重要，就可以让我们明白，就是符合条件的用户，他最感兴趣的商品是什么种类。这个可以让公司里的人，清晰地了解到不同层次、不同类型的用户的心理和喜好。
  
  5、对于排名前10的品类，分别获取其点击次数排名前10的session
  
  这个就是说，对于top10的品类，每一个都要获取对它点击次数排名前10的session。
  
  这个功能，可以让我们看到，对某个用户群体最感兴趣的品类，各个品类最感兴趣最典型的用户的session的行为。

####模块二：页面单跳转化率
> 需求：
1，接收taskid，从mysql中查询任务的参数，日期范围，页面流id；
2，针对指定范围日期内的用户行为数据，判断和计算，页面流id中，每两个页面组成的页面切片，它的访问量是多少；
3，根据指定页面流中各个页面切片的访问量，计算出各个页面切片的转化率
4，计算出的转化率写入MySQL；
    1，页面单跳转化率业务，实现思路；
    2，页面单跳切片生成及页面流匹配。